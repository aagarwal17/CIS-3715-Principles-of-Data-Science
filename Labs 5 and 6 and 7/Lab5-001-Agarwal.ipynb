{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5: Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, we will learn the Logistic Regression model.\n",
    "\n",
    "First, please study the given example, which uses the logistic regression model for the breast cancer classification task. In this example, you will learn how to preprocess data, how to train the model, and how to evaluate the model.\n",
    "\n",
    "Based on the given example, your task is to use the logistic regression model to predict the presence of heart disease.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load the breast cancer data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we use the [breast cancer](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html#sklearn.datasets.load_breast_cancer) dataset in sklearn. It is a binary classification dataset. Each sample has 30 numerical features, which can be found in [7.1.7](https://scikit-learn.org/stable/datasets/toy_dataset.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#samples: 569, #features: 30\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from random import shuffle\n",
    "import random\n",
    "random.seed(123)\n",
    "\n",
    "X, y = datasets.load_breast_cancer(return_X_y=True)\n",
    "print(\"#samples: {}, #features: {}\".format(X.shape[0], X.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Split the data into two subsets and normalize the features of samples\n",
    "\n",
    "Here, we use 69 samples as the testing set and use the remained samples to train the logistic regression model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_val: 500, test: 69\n"
     ]
    }
   ],
   "source": [
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, \n",
    "                                                            test_size=0.12, \n",
    "                                                            random_state=0)\n",
    "print(\"train_val: {}, test: {}\".format(X_train_val.shape[0], X_test.shape[0]))\n",
    "\n",
    "normalizer = StandardScaler()\n",
    "X_train_val = normalizer.fit_transform(X_train_val)\n",
    "X_test = normalizer.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Train the logistic regression model and select the hyperparameter with cross-validation\n",
    "\n",
    "Here, we use the following logistic regression model to do cancer classification. \n",
    "\n",
    "\\begin{equation}\n",
    "\t\\min_{\\mathbf{w}} \\sum_{i=1}^{n}\\{\\log(1+\\exp(\\mathbf{w}^T\\mathbf{x}_i))-y_i\\mathbf{w}^T\\mathbf{x}_i \\} + \\lambda\\|\\mathbf{w}\\|_2^2\n",
    "\\end{equation}\n",
    "\n",
    "We need to learn the model parameter $\\mathbf{w}$. However, with different hyperparameters $\\lambda$, we can get different model parameter $\\mathbf{w}$, resulting in different prediction performance. Here, we use the 5-fold cross-validation to select the hyperparameter $\\lambda$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[380 468 145   2 282 103 148 312 130 411 168 204 113 132 418 270 351 157\n",
      "  451 339 288 277 354  48 318 303 314 234  95 304 271 434 173 357 495 133\n",
      "  431  39 490 310 317 471  23 426 224 286  20 365 255 216 405  79 228 445\n",
      "  189 184 243 358 276 218 488  60 438 159 167 349  89 121 333  51   9 482\n",
      "  152 416 379 306 111 185 340 489 475  93  84 376 291 158 250 323 406 460\n",
      "   50 433 372  66 108 465  71 298 369 437]\n",
      " [211  11 110 142  28  59 163  38  24 205 440 140 177 252 235 245 242  25\n",
      "   21 217 160 231  77 151  54 345 280 257 456 308 331  58 360 179 464 388\n",
      "  129 285 347  56 387 169  36 138 319 296 246 122  33 127 109 363 183 196\n",
      "  422  86 400 297 346 116  63  88 477 144 112 362 399 334  62 353 146 373\n",
      "   27  76 260 150 210 195 290  82 154 432 320 361  75  17  94 238 143 469\n",
      "   67 225 391 106  15  97  46  49 192 226]\n",
      " [114 302 356  91  80 107 329 209 384 409  13 176 299 483 295 491 332 292\n",
      "  153 202 268   1 417 313 375 128 352  57 408 254 382 390 377 328 213 182\n",
      "   65   7 315 101 187 126 123 394 201 251 494 239 383 367 237  34 307 141\n",
      "  403 344 162  43 118 498  99 392 102 258 100  41 281 364 492 448 164 104\n",
      "  124 259 355 458 484 115 309 338  53 381 442  70 284 263 419 166 441 481\n",
      "  335 219 155 294 230 378 476 232 480  31]\n",
      " [343 197 301  85  61 264 446 273 455 188 199 452  74 443 423 395 265  29\n",
      "   40 120 190  73 348 415 474 337  12 178 212 402 478 412 241 454 165  14\n",
      "  206 325 279 398 366 462   4 221 421 389 181 413  32 316 493 473 215 324\n",
      "  425 139 424 385 131 453  98 470  68   5 459 236 466 227 487  78  90 439\n",
      "  278 119 368 322 253 147 435  30 397 256 272 207 117 180 430 186 321  45\n",
      "  300  96   8 401 450 198 233 370  37 200]\n",
      " [283 479 171  87 134 336 249  42 371  92 427 386  16 261 191 214 342 266\n",
      "  248 467 457 407 326 275 350 222 262 330 444 203   6 472 414 289 269 327\n",
      "  311 420 105 247 410 267 175 156 496  18 428 240 135 244 293 220 149  10\n",
      "  404  64  72 341  47  22  52 229 374 161   3  35 193 305 449 497 396 223\n",
      "  463   0  83 125 359 485 486 172  69  81 499 436 174 170 287 274 194  19\n",
      "  447 461 429  55 136 208 393  44 137  26]]\n",
      "reg_coeff: 10.0, acc: 0.970\n",
      "reg_coeff: 2.0, acc: 0.978\n",
      "reg_coeff: 1.0, acc: 0.972\n",
      "reg_coeff: 0.2, acc: 0.968\n",
      "reg_coeff: 0.1, acc: 0.968\n"
     ]
    }
   ],
   "source": [
    "# here we use 5-fold cross-validation\n",
    "folds = 5\n",
    "\n",
    "# get the number of samples in the training and validation set\n",
    "num_train_val = X_train_val.shape[0] \n",
    "\n",
    "# shuffle the index of samples in the train_val set\n",
    "index_of_samples = np.arange(num_train_val) \n",
    "shuffle(index_of_samples)\n",
    "\n",
    "# split the index of the train_valid set into 5 folds\n",
    "index_of_folds = index_of_samples.reshape(folds, -1)\n",
    "print(index_of_folds)\n",
    "\n",
    "# potential hyperparameters. \n",
    "#These hyperparameters are just used for illustration. \n",
    "#You should try more hyperparameters to get a good model.\n",
    "#The hyperparameters must be nonnegative!\n",
    "regularization_coefficient = [0.1, 0.5, 1.0, 5.0, 10.0]\n",
    "\n",
    "best_acc = 0.0\n",
    "best_reg = 0.0\n",
    "\n",
    "for reg in regularization_coefficient:\n",
    "    \n",
    "    # 5-fold cross-validation\n",
    "    sum_acc = 0.0\n",
    "    for fold in range(folds):\n",
    "        \n",
    "        index_of_folds_temp = index_of_folds.copy()\n",
    "        \n",
    "        valid_index = index_of_folds_temp[fold,:].reshape(-1) #get the index of the validation set\n",
    "        train_index = np.delete(index_of_folds_temp, fold, 0).reshape(-1) #get the index of the training set\n",
    "        \n",
    "        # training set\n",
    "        X_train = X_train_val[train_index]\n",
    "        y_train = y_train_val[train_index]\n",
    "        \n",
    "        # validation set\n",
    "        X_valid = X_train_val[valid_index]\n",
    "        y_valid = y_train_val[valid_index]\n",
    "                \n",
    "        # build the model with different hyperparameters\n",
    "        clf = LogisticRegression(penalty='l2', C=reg, solver='lbfgs')\n",
    "        \n",
    "        #train the model with the training set\n",
    "        clf.fit(X_train, y_train)\n",
    "        \n",
    "        y_valid_pred = clf.predict(X_valid)\n",
    "        acc = accuracy_score(y_valid, y_valid_pred)\n",
    "        \n",
    "        sum_acc += acc\n",
    "    \n",
    "    cur_acc = sum_acc / folds\n",
    "    \n",
    "    print(\"reg_coeff: {}, acc: {:.3f}\".format(1.0/reg, cur_acc))\n",
    "    \n",
    "    # store the best hyperparameter\n",
    "    if cur_acc > best_acc:\n",
    "        best_acc = cur_acc\n",
    "        best_reg = reg\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Evaluate the learned model\n",
    "\n",
    "After getting the best hyperparameter $\\lambda$, we retrain the model with the train_val set. Then, we evaluate this  model on the testing set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 1.000, recall: 1.000, precision: 1.000, f1: 1.000,\n"
     ]
    }
   ],
   "source": [
    "# retrain the model\n",
    "clf = LogisticRegression(penalty='l2', C=best_reg, solver='lbfgs')\n",
    "clf.fit(X_train_val, y_train_val)\n",
    "\n",
    "# evaluate the model on the testing set\n",
    "y_test_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "acc = accuracy_score(y_test, y_test_pred)\n",
    "f1 = f1_score(y_test, y_test_pred)\n",
    "recall = recall_score(y_test, y_test_pred)\n",
    "precision = precision_score(y_test, y_test_pred)\n",
    "\n",
    "\n",
    "print(\"accuracy: {:.3f}, recall: {:.3f}, precision: {:.3f}, f1: {:.3f},\".format(acc, recall, precision, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Task\n",
    "\n",
    "Here, we use the [heart disease](./heart.csv) dataset. Each sample has the following feature: \n",
    "\n",
    "* age\n",
    "* sex\n",
    "* chest pain type (4 values)\n",
    "* resting blood pressure\n",
    "* serum cholestoral in mg/dl\n",
    "* fasting blood sugar > 120 mg/dl\n",
    "* resting electrocardiographic results (values 0,1,2)\n",
    "* maximum heart rate achieved\n",
    "* exercise induced angina\n",
    "* oldpeak = ST depression induced by exercise relative to rest\n",
    "* the slope of the peak exercise ST segment\n",
    "* number of major vessels (0-3) colored by flourosopy\n",
    "* thal: 3 = normal; 6 = fixed defect; 7 = reversable defect\n",
    "\n",
    "The last column refers to the presence of heart disease in the patient.\n",
    "\n",
    "The task is to predict whether a person has the heart disease. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Preprocess the raw data\n",
    "\n",
    "* Check whether there are missing values\n",
    "* Check whether theare are cateogrical features\n",
    "* Check whether this dataset is balanced or not (use the bar plot to visualize the number of positive and negative samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age         0\n",
       "sex         0\n",
       "cp          0\n",
       "trestbps    0\n",
       "chol        0\n",
       "fbs         0\n",
       "restecg     0\n",
       "thalach     0\n",
       "exang       0\n",
       "oldpeak     0\n",
       "slope       0\n",
       "ca          0\n",
       "thal        0\n",
       "target      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#When given a dataset, we must first deal with the missing values and any categorical features\n",
    "\n",
    "#We may not need all these imports, but I am including them just in case\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.metrics import mean_absolute_error,mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv(\"heart.csv\")\n",
    "\n",
    "#First, we will determine whether there are any missing values:\n",
    "df.isnull().sum()\n",
    "#Since the sum of the null values for each feature is zero for every feature, there are no missing values in this dataset\n",
    "#We do not need to fill in any missing values!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 303 entries, 0 to 302\n",
      "Data columns (total 14 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   age       303 non-null    int64  \n",
      " 1   sex       303 non-null    int64  \n",
      " 2   cp        303 non-null    int64  \n",
      " 3   trestbps  303 non-null    int64  \n",
      " 4   chol      303 non-null    int64  \n",
      " 5   fbs       303 non-null    int64  \n",
      " 6   restecg   303 non-null    int64  \n",
      " 7   thalach   303 non-null    int64  \n",
      " 8   exang     303 non-null    int64  \n",
      " 9   oldpeak   303 non-null    float64\n",
      " 10  slope     303 non-null    int64  \n",
      " 11  ca        303 non-null    int64  \n",
      " 12  thal      303 non-null    int64  \n",
      " 13  target    303 non-null    int64  \n",
      "dtypes: float64(1), int64(13)\n",
      "memory usage: 33.3 KB\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Next, we will convert the categorical features to numerical values\n",
    "#To determine which features are categorical, we will use the following method:\n",
    "df.info()\n",
    "#From this, we determine that there are no categorical features.\n",
    "\n",
    "#To check whether the dataset is balanced or not, we will use the barplot to visualize the number of positive and negative samples.\n",
    "#We are simply plotting the numbers of target values as 1 and number of target values as 0\n",
    "df['target'].value_counts().plot(kind = 'bar')\n",
    "plt.title('Bar Graph of Positive and Negative Samples')\n",
    "plt.show()\n",
    "#Based on this bar plot, we can see that the dataset is not perfectly balanced, but is pretty close (there are more samples with values of 1 than 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Split the data into two subsets and normalize the features of samples\n",
    "\n",
    "* Split the dataset into the train_val set and testing set. \n",
    "* Normalize features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_val: 260, test: 43\n",
      "(43, 13)\n",
      "(260, 13)\n"
     ]
    }
   ],
   "source": [
    "#Splitting the samples:\n",
    "target_fea = df.drop('target', axis=1).values\n",
    "target_prediction = df['target'].values\n",
    "\n",
    "\n",
    "#Here, we use 14% of samples as the testing set and use the remaining samples to train the logistic regression model.\n",
    "#Thus, there are 43 of the samples in the testing set and 260 of the samples to train the logistic regression model. This is important for later so that we can evenly split into 10 folds.\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(target_fea, target_prediction, \n",
    "                                                            test_size=0.14, \n",
    "                                                            random_state=0)\n",
    "print(\"train_val: {}, test: {}\".format(X_train_val.shape[0], X_test.shape[0]))\n",
    "\n",
    "#Normalizing the features:\n",
    "normalizer = StandardScaler()\n",
    "X_train_val = normalizer.fit_transform(X_train_val)\n",
    "X_test = normalizer.transform(X_test)\n",
    "\n",
    "print(X_test.shape)\n",
    "print(X_train_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Train the logistic regression model and select the hyperparameter with cross-validation\n",
    "\n",
    "\\begin{equation}\n",
    "\t\\min_{\\mathbf{w}} \\sum_{i=1}^{n}\\{\\log(1+\\exp(\\mathbf{w}^T\\mathbf{x}_i))-y_i\\mathbf{w}^T\\mathbf{x}_i \\} + \\lambda\\|\\mathbf{w}\\|_2^2\n",
    "\\end{equation}\n",
    "\n",
    "* Use the 10-fold cross-validation to select the hyperparameter $\\lambda$.\n",
    "* Search $\\lambda$ from $\\{10^{-5}, 10^{-4}, 10^{-3}, 10^{-2}, 10^{-1}, 1, 10, 20, 50, 100\\}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 92  68 103  28   7 178 212   9 215 209 136 150  72 101  51 131 128  12\n",
      "    1  41 108  53  99 256 148 232]\n",
      " [ 30 154 221  90 237  57 259  22 250 223 242  79 193  11  64 138 204 217\n",
      "  145 251 245  43 230  85 146 219]\n",
      " [149  15 165 227 151 191  45 184 225 213   4 244  24 210  48  96  87  84\n",
      "  114 173 207 124  29  78   3 211]\n",
      " [180 134  91 247 226   6  55 201 168  17 240 152  67 216 181  20 229 183\n",
      "   65  47  62 111 141 161 147  89]\n",
      " [162 185  36 194 133 220 190  80  75 228  31  52 142  38 104 110  98 105\n",
      "   25  81  21 238 188 241 143 208]\n",
      " [ 94 170 186 199  27 195 202 174 233 254  63 222 253 157 163  35  73 120\n",
      "  192  76 252 155   8 175  54 179]\n",
      " [ 50 100 249 172  74 127 160 198 122 129 137 130  18  16 236  56  49  33\n",
      "   95 176 159 187 126 132  26  58]\n",
      " [109 258  37 224 214  10 158 123 139 255  71 106  42  93  23  77 166 112\n",
      "  196 189  59  19 234 203  83 169]\n",
      " [177 218 235 107 144  97  14 121 140 102 200  70 164  44  32 135  61  40\n",
      "  257 197 239 231 125  46  69 205]\n",
      " [ 34  66   5  60  13 171 116  86 119 167   0 248  82 156 182 206 113  88\n",
      "  153 246  39 117 115 243 118   2]]\n",
      "reg_coeff: 99999.99999999999, acc: 0.554\n",
      "reg_coeff: 1000.0, acc: 0.696\n",
      "reg_coeff: 100.0, acc: 0.831\n",
      "reg_coeff: 10.0, acc: 0.823\n",
      "reg_coeff: 1.0, acc: 0.831\n",
      "reg_coeff: 0.1, acc: 0.831\n",
      "reg_coeff: 0.05, acc: 0.831\n",
      "reg_coeff: 0.02, acc: 0.831\n",
      "reg_coeff: 0.01, acc: 0.831\n",
      "Best Accuracy: 0.8308 \n",
      "Best Reg: 0.01\n"
     ]
    }
   ],
   "source": [
    "#We need to learn the model parameter  𝐰 . \n",
    "#However, with different hyperparameters  𝜆 , we can get different model parameter  𝐰 , resulting in different prediction performance. \n",
    "#Thus, we will use the 10-fold cross-validation to select the hyperparameter  𝜆 .\n",
    "\n",
    "#Here we set the folds equal to 10 for 10-fold cross-validation\n",
    "folds = 10\n",
    "\n",
    "#We get the number of samples in the training and validation set\n",
    "num_train_val = X_train_val.shape[0] \n",
    "\n",
    "#Now, we shuffle the index of samples in the train_val set\n",
    "index_of_samples = np.arange(num_train_val) \n",
    "shuffle(index_of_samples)\n",
    "\n",
    "#We split the index of the train_valid set into 10 folds\n",
    "index_of_folds = index_of_samples.reshape(folds, -1)\n",
    "print(index_of_folds)\n",
    "\n",
    "#As suggested above, the hyperparameters chosen are listed below\n",
    "regularization_coefficient = [10**(-5), 10**(-3), 10**(-2), 10**(-1), 1, 10, 20, 50, 100]\n",
    "\n",
    "#Variables we create to store the values of the best accuracy and best regression:\n",
    "best_acc = 0.0\n",
    "best_reg = 0.0\n",
    "\n",
    "for reg in regularization_coefficient:\n",
    "    #10-fold cross-validation\n",
    "    sum_acc = 0.0\n",
    "    for fold in range(folds):\n",
    "        \n",
    "        index_of_folds_temp = index_of_folds.copy()\n",
    "        \n",
    "        #We are getting the index of the validation set and storing it in a variable valid_index\n",
    "        valid_index = index_of_folds_temp[fold,:].reshape(-1) \n",
    "        #We are getting the index of the training set and storing it in a variable train_index\n",
    "        train_index = np.delete(index_of_folds_temp, fold, 0).reshape(-1)\n",
    "        \n",
    "        #Our training set:\n",
    "        X_train = X_train_val[train_index]\n",
    "        y_train = y_train_val[train_index]\n",
    "        \n",
    "        #Our validation set:\n",
    "        X_valid = X_train_val[valid_index]\n",
    "        y_valid = y_train_val[valid_index]\n",
    "                \n",
    "        #We write this to build the model with different hyperparameters:\n",
    "        clf = LogisticRegression(penalty='l2', C=reg, solver='lbfgs')\n",
    "        \n",
    "        #Train the model with the training set:\n",
    "        clf.fit(X_train, y_train)\n",
    "        \n",
    "        y_valid_pred = clf.predict(X_valid)\n",
    "        acc = accuracy_score(y_valid, y_valid_pred)\n",
    "        \n",
    "        sum_acc += acc\n",
    "    \n",
    "    cur_acc = sum_acc / folds\n",
    "    \n",
    "    print(\"reg_coeff: {}, acc: {:.3f}\".format(1.0/reg, cur_acc))\n",
    "    \n",
    "    #We now want to store the best hyperparameter:\n",
    "    if cur_acc > best_acc:\n",
    "        best_acc = cur_acc\n",
    "        best_reg = reg\n",
    "        \n",
    "print(\"Best Accuracy: {:.4f} \".format(best_acc))\n",
    "print(\"Best Reg: {:}\".format(best_reg))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Evaluate the learned model\n",
    "\n",
    "* Report the prediction accuracy, recall, precision, and F1 score.\n",
    "\n",
    "* Use the bar plot to visulaize the elements of the learned model parameter vector $\\mathbf{w}$. Some elements  have larger absolute values, while the others do not. Try to explain this phenomenon. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.791, recall: 0.905, precision: 0.731, f1: 0.809,\n",
      "Our learned model parameter vector 'w':  [0.07260054 0.20696578 0.25200775 0.07372149 0.0360397  0.02899053\n",
      " 0.03336877 0.17880825 0.21720207 0.20099102 0.13853072 0.26740514\n",
      " 0.1796196 ]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Now that we have the best hyperparameter, we retrain the model:\n",
    "clf = LogisticRegression(penalty='l2', C=best_reg, solver='lbfgs')\n",
    "clf.fit(X_train_val, y_train_val)\n",
    "\n",
    "# evaluate the model on the testing set\n",
    "y_test_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "acc = accuracy_score(y_test, y_test_pred)\n",
    "f1 = f1_score(y_test, y_test_pred)\n",
    "recall = recall_score(y_test, y_test_pred)\n",
    "precision = precision_score(y_test, y_test_pred)\n",
    "\n",
    "#Visualizing the elements of the learned model parameter vector w with a bar plot:\n",
    "print(\"accuracy: {:.3f}, recall: {:.3f}, precision: {:.3f}, f1: {:.3f},\".format(acc, recall, precision, f1))\n",
    "\n",
    "print(\"Our learned model parameter vector 'w': \", abs(clf.coef_[0]))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "labels = ['Age','Sex','cp','trestbps','chol','fbs','restecg','thalach','exang','oldpeak','slope','ca','thal']\n",
    "coefficients = abs(clf.coef_[0])\n",
    "ax.bar(labels, coefficients)\n",
    "plt.title(\"Bar Graph of Elements of the Learned Model Parameter Vector 'w'\")\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Coefficients')\n",
    "plt.show()\n",
    "\n",
    "#From this plot, we can see that some features have much greater coefficients in the learned model parameter than others.\n",
    "#Specifically, Sex, cp (chest pain type), ca (the number of major vessels) have the higher coefficient values.\n",
    "#This means that these features were able to fit the logistic regression model with the hyperparameter best of all of the features. \n",
    "#It tells us that the features 'Sex', 'cp', and 'ca' in our model are more statistically significant than the other features. The closer the coefficient values are to 0, the less correlation exists (that is why we took the absolute value)\n",
    "\n",
    "#It should be noted that, although we took the absolute value of the coefficients, a positive coefficient indicates that as the value of the independent variable increases, the mean of the dependent variable also tends to increase. \n",
    "#A negative coefficient suggests that as the independent variable increases, the dependent variable tends to decrease."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
